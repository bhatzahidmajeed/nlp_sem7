{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Q1 Write a python code to remove punctuations, URLs and stop words."
      ],
      "metadata": {
        "id": "64azR8vULIWS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIJOq6jiK91o",
        "outputId": "a83027c1-4ea7-4dad-a3fe-f1f22c417fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_input = \"I love reading articles on AI, ML, and robotics. One of my favorite websites for this is https://www.example-ai-ml.com. The content there is fantastic! But sometimes, the articles can be a bit lengthy and repetitive. I often visit the website to learn new things and improve my knowledge in these fields.\""
      ],
      "metadata": {
        "id": "NRmC-98RNhZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuations(text):\n",
        "    # Remove punctuations using regular expressions\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "# Remove punctuations\n",
        "cleaned_text = remove_punctuations(example_input)\n",
        "print(\"Text without punctuations:\")\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lNrwfb-Mz-j",
        "outputId": "59f6b275-fa4a-4e4b-d355-fbd15191566d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without punctuations:\n",
            "I love reading articles on AI ML and robotics One of my favorite websites for this is httpswwwexampleaimlcom The content there is fantastic But sometimes the articles can be a bit lengthy and repetitive I often visit the website to learn new things and improve my knowledge in these fields\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    # Remove URLs using regular expressions\n",
        "    cleaned_text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "# Remove URLs\n",
        "cleaned_text = remove_urls(cleaned_text)\n",
        "print(\"Text without URLs:\")\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBvqprZjM358",
        "outputId": "b137e9f4-30be-459f-c6f2-9d3818bd54ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without URLs:\n",
            "I love reading articles on AI ML and robotics One of my favorite websites for this is  The content there is fantastic But sometimes the articles can be a bit lengthy and repetitive I often visit the website to learn new things and improve my knowledge in these fields\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(text):\n",
        "    # Remove stop words using nltk library\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
        "    cleaned_text = ' '.join(cleaned_words)\n",
        "    return cleaned_text\n",
        "\n",
        "# Remove stop words\n",
        "cleaned_text = remove_stop_words(cleaned_text)\n",
        "print(\"Text without stop words:\")\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I16nBIL5M9nH",
        "outputId": "64c962f6-30c7-4572-b180-fd11ce67b7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without stop words:\n",
            "love reading articles AI ML robotics One favorite websites content fantastic sometimes articles bit lengthy repetitive often visit website learn new things improve knowledge fields\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2 Write a python code perform stemmer operation using Porterstemmer, Snowballstemmer, Lancasterstemmer, RegExpStemmer"
      ],
      "metadata": {
        "id": "gVOR8f_HPXKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "nNBc6nxUPgq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "words = [\"Connects\" ,\"Connecting\",\"Connections\",\"Connected\",\"Connection\",\"Connectings\",\"Connect\"]\n",
        "for word in words:\n",
        "  print(word,\"\\t---> \",porter.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKZdPUtkQ8pr",
        "outputId": "1e020e82-4d86-470b-da89-395940752859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connects \t--->  connect\n",
            "Connecting \t--->  connect\n",
            "Connections \t--->  connect\n",
            "Connected \t--->  connect\n",
            "Connection \t--->  connect\n",
            "Connectings \t--->  connect\n",
            "Connect \t--->  connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowball = SnowballStemmer(language=\"english\")\n",
        "words = [\"generous\",\"generate\",\"generously\",\"generation\"]\n",
        "for word in words:\n",
        "  print(word,\"\\t---> \",snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JztedcrRStja",
        "outputId": "0c6176ee-72ca-4689-e9ee-f07ece9020fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generous \t--->  generous\n",
            "generate \t--->  generat\n",
            "generously \t--->  generous\n",
            "generation \t--->  generat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lancaster = LancasterStemmer()\n",
        "words = [\"eating\",\"eats\",\"eaten\",\"puts\",\"putting\"]\n",
        "for word in words:\n",
        "  print(word,\"\\t---> \",lancaster.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RkK6vKIUx5a",
        "outputId": "50fd3682-c9b7-4e62-ba6a-b4f1456bfea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating \t--->  eat\n",
            "eats \t--->  eat\n",
            "eaten \t--->  eat\n",
            "puts \t--->  put\n",
            "putting \t--->  put\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "words = ['mass','was','bee','computer','advisable']\n",
        "for word in words:\n",
        "  print(word,\"\\t--->\",regexp.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fomNJQgVKyL",
        "outputId": "158cad8f-5ef4-4dd4-a394-ab337f494300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mass \t---> mas\n",
            "was \t---> was\n",
            "bee \t---> bee\n",
            "computer \t---> computer\n",
            "advisable \t---> advis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3 Write a python code to demonstrate the comparative study of all 4 stemmers for a given text corpus."
      ],
      "metadata": {
        "id": "caRQC9uWaime"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer(language=\"english\")\n",
        "lancaster = LancasterStemmer()\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "words = ['computer','advisable','eating','dancing','generous','joyfully']\n",
        "stemmed_words = []\n",
        "for word in words:\n",
        "  stemmed_words.append([word, porter.stem(word), snowball.stem(word), lancaster.stem(word), regexp.stem(word)])\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "headers = [\"WORD\", 'PORTER STEM', 'SNOWBALL STEM', 'LANCASTER STEM', 'REGEXP STEM']\n",
        "print(tabulate(stemmed_words, headers = headers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKajg0P-Vmxe",
        "outputId": "bcd299f3-4224-4da2-91b0-bf931244845a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORD       PORTER STEM    SNOWBALL STEM    LANCASTER STEM    REGEXP STEM\n",
            "---------  -------------  ---------------  ----------------  -------------\n",
            "computer   comput         comput           comput            computer\n",
            "advisable  advis          advis            adv               advis\n",
            "eating     eat            eat              eat               eat\n",
            "dancing    danc           danc             dant              danc\n",
            "generous   gener          generous         gen               generou\n",
            "joyfully   joy            joy              joy               joyfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4 Write a python code perform lemmatization using NLTK library."
      ],
      "metadata": {
        "id": "1B_Hz8q_X5vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "from tabulate import tabulate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GefmZwW0YNiL",
        "outputId": "99f7019c-7fa8-4497-fc69-84ed1fb64a14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "words = ['dances', 'employees', 'rashes', 'experiments', 'converts']\n",
        "headers = ['word', 'lemmatized word']\n",
        "lemm_words = []\n",
        "for word in words:\n",
        "  lemm_words.append([word,wnl.lemmatize(word)])\n",
        "print(tabulate(lemm_words, headers = headers))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMy9jKDNZZKg",
        "outputId": "71cc496b-ccac-4471-b644-a60294d256a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word         lemmatized word\n",
            "-----------  -----------------\n",
            "dances       dance\n",
            "employees    employee\n",
            "rashes       rash\n",
            "experiments  experiment\n",
            "converts     convert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5 Write a python code perform lemmatization using Spacy library."
      ],
      "metadata": {
        "id": "mQhBuS9maWWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "IXeV5K3Oaz3J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "words = ['happening', 'employments', 'rashes', 'experimental', 'conversion']\n",
        "lemm_words = []\n",
        "for word in words:\n",
        "  doc = nlp(word)\n",
        "  lemm_words.append([word, doc[0].lemma_])\n",
        "print(tabulate(lemm_words, headers = headers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFFOA1uVcv8J",
        "outputId": "315563b8-011a-4cf0-ad40-a61b8e9722c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word          lemmatized word\n",
            "------------  -----------------\n",
            "happening     happen\n",
            "employments   employment\n",
            "rashes        rashe\n",
            "experimental  experimental\n",
            "conversion    conversion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q6 Compare the results lemmatization with Spacy and NLTK for the corpus given below-\n",
        "##walking, is , main, animals , foxes, are, jumping , sleeping.\n",
        "##Write your conclusion for the results obtained."
      ],
      "metadata": {
        "id": "9wNQJYNNeJtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "words = ['walking', 'is ', 'main', 'animals' , 'foxes', 'are', 'jumping ', 'sleeping']\n",
        "lemm_words = []\n",
        "headers = [\"WORD\", 'NLTK (ADJECTIVE)', 'NLTK (NOUN)', 'NLTK (VERB)', 'NLTK (ADVERB)', 'SPACY']\n",
        "for word in words:\n",
        "  doc = nlp(word)\n",
        "  lemm_words.append([word, wnl.lemmatize(word, pos = 'a'), wnl.lemmatize(word, pos = 'n'), wnl.lemmatize(word, pos = 'v'), wnl.lemmatize(word, pos = 'r'), doc[0].lemma_])\n",
        "\n",
        "print(tabulate(lemm_words, headers = headers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ewzz60HeX20",
        "outputId": "59981e19-ebef-4ae4-b6a6-0bbbe3f16b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORD      NLTK (ADJECTIVE)    NLTK (NOUN)    NLTK (VERB)    NLTK (ADVERB)    SPACY\n",
            "--------  ------------------  -------------  -------------  ---------------  -------\n",
            "walking   walking             walking        walk           walking          walk\n",
            "is        is                  is             is             is               be\n",
            "main      main                main           main           main             main\n",
            "animals   animals             animal         animals        animals          animal\n",
            "foxes     foxes               fox            fox            foxes            fox\n",
            "are       are                 are            be             are              be\n",
            "jumping   jumping             jumping        jumping        jumping          jump\n",
            "sleeping  sleeping            sleeping       sleep          sleeping         sleep\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Post Lab Questions:\n",
        "## What all python Libraries are available to work with Indian languages like Hindi, Punjabi, Marathi..etc?"
      ],
      "metadata": {
        "id": "8Z5fOAsWL-oK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Marathi"
      ],
      "metadata": {
        "id": "6KrMP2SEXgoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inltk --quiet\n",
        "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrDZJD1oWKYB",
        "outputId": "7c50e720-2f7e-4422-cc96-c3b13bcde76c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.3.1+cpu (from versions: 1.11.0, 1.11.0+cpu, 1.11.0+cu102, 1.11.0+cu113, 1.11.0+cu115, 1.11.0+rocm4.3.1, 1.11.0+rocm4.5.2, 1.12.0, 1.12.0+cpu, 1.12.0+cu102, 1.12.0+cu113, 1.12.0+cu116, 1.12.0+rocm5.0, 1.12.0+rocm5.1.1, 1.12.1, 1.12.1+cpu, 1.12.1+cu102, 1.12.1+cu113, 1.12.1+cu116, 1.12.1+rocm5.0, 1.12.1+rocm5.1.1, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.0+rocm5.1.1, 1.13.0+rocm5.2, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 1.13.1+cu117.with.pypi.cudnn, 1.13.1+rocm5.1.1, 1.13.1+rocm5.2, 2.0.0, 2.0.0+cpu, 2.0.0+cpu.cxx11.abi, 2.0.0+cu117, 2.0.0+cu117.with.pypi.cudnn, 2.0.0+cu118, 2.0.0+rocm5.3, 2.0.0+rocm5.4.2, 2.0.1, 2.0.1+cpu, 2.0.1+cpu.cxx11.abi, 2.0.1+cu117, 2.0.1+cu117.with.pypi.cudnn, 2.0.1+cu118, 2.0.1+rocm5.3, 2.0.1+rocm5.4.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.3.1+cpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections.abc\n",
        "collections.Iterable = collections.abc.Iterable\n",
        "collections.Mapping = collections.abc.Mapping\n",
        "collections.MutableSet = collections.abc.MutableSet\n",
        "collections.MutableMapping = collections.abc.MutableMapping"
      ],
      "metadata": {
        "id": "dT603qUzW0y_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inltk.inltk import setup\n",
        "setup('mr')"
      ],
      "metadata": {
        "id": "kqNJpQ9-YAPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inltk\n",
        "from inltk.inltk import tokenize\n",
        "marathi_text = \"केलेल्या अनेक कष्टांना विचारता आपले जीवन सुंदर झाले, याचं सारंगी आपल्या मनाला सुगंधित करणारं एक आनंददायी अनुभव आहे.\"\n",
        "tokens = tokenize(marathi_text, \"mr\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "561xSEnIP-PO",
        "outputId": "ee54908f-4505-458d-b8c5-bad0942f5a9c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁केलेल्या', '▁अनेक', '▁कष्ट', 'ांना', '▁विचार', 'ता', '▁आपले', '▁जीवन', '▁सुंदर', '▁झाले', ',', '▁या', 'चं', '▁सारंगी', '▁आपल्या', '▁मनाला', '▁सुगंध', 'ित', '▁करणार', 'ं', '▁एक', '▁आनंददाय', 'ी', '▁अनुभव', '▁आहे', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hindi"
      ],
      "metadata": {
        "id": "AScSHvE3XqM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from inltk.inltk import setup\n",
        "setup('hi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "gJ6OLHMzX8fE",
        "outputId": "172d8155-7f47-4e7e-daf5-65f82e42a79a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ea8b68f1926d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/inltk/inltk.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(language_code)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \"\"\"\n\u001b[1;32m    624\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Model. This might take time, depending on your internet connection. Please be patient.\n",
            "We'll only do this for the first time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inltk\n",
        "from inltk.inltk import tokenize\n",
        "hindi_text = \"मैंने अपने दोस्त को पुरानी यादें ताजा करने के लिए एक साथीक यात्रा पर बुलाया, जहां हमने बहुत मजेदार और रोमांचक लम्हों को जीवंत किया।\"\n",
        "tokens = tokenize(hindi_text, \"hi\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTAM2A3OXs-6",
        "outputId": "c08de651-d1bc-4327-a86e-85a6f429ed72"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁मैंने', '▁अपने', '▁दोस्त', '▁को', '▁पुरानी', '▁याद', 'ें', '▁ताजा', '▁करने', '▁के', '▁लिए', '▁एक', '▁साथ', 'ीक', '▁यात्रा', '▁पर', '▁बुलाया', ',', '▁जहां', '▁हमने', '▁बहुत', '▁मजेदार', '▁और', '▁रोमांचक', '▁लम्', 'हों', '▁को', '▁जीवंत', '▁किया', '।']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gujarati"
      ],
      "metadata": {
        "id": "6SXWqoZlYL23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from inltk.inltk import setup\n",
        "setup('gu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "_7uJC0FFYP4l",
        "outputId": "aaee8299-fe93-4fab-8fd1-c5dd81b5dfec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-01c46dc1238c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/inltk/inltk.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(language_code)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \"\"\"\n\u001b[1;32m    624\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inltk\n",
        "from inltk.inltk import tokenize\n",
        "gujarati_text = \"મારા મિત્રોને જુઓની યાદોને તાજી કરવા માટે મારી સાથીની યાત્રાએ બોજ મોજનારા અને ઉલ્લાસદાયી પલાઓ જીવંત બનાવ્યા.\"\n",
        "tokens = tokenize(gujarati_text , \"gu\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6TIDd4yYTCI",
        "outputId": "332b08e7-4f13-44b5-da4d-fdaf5c030954"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁મારા', '▁મિત્રો', 'ને', '▁જુઓ', 'ની', '▁યાદ', 'ોને', '▁તાજી', '▁કરવા', '▁માટે', '▁મારી', '▁સાથી', 'ની', '▁યાત્રા', 'એ', '▁બોજ', '▁મોજ', 'નારા', '▁અને', '▁ઉલ્લાસ', 'દા', 'યી', '▁પલા', 'ઓ', '▁જીવંત', '▁બનાવ્યા', '.']\n"
          ]
        }
      ]
    }
  ]
}